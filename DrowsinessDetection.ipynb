{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drowsiness Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The National Highway Traffic Safety Administration (NHTSA) estimated in 2017 that nearly 100,000 police-reported crashes involved drowsy drivers. Those crashes led to an estimated 50,000 injuries and 800 deaths. There is a broad agreement between the traffic safety, sleep science, and public health communities that even these statistics are understated.](https://www.nhtsa.gov/risky-driving/drowsy-driving) Therefore, the value of drowsiness detection is immeasureable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting the drowsiness of a person could be extremely valuable for protecting the lives of drivers, especially truck drivers. It is a difficult and rather nuanced problem, however, that involves facial recognition algorithms, simple geometry, and logic. \n",
    "\n",
    "First of all, we're going to want to consider physical determinants of drowsiness that can be expressed mathemtically. The option that I'm choosing to go with is tracking eye movements in order to measure the duration that the eye is closed. If they're closed beyond a certain number of frames, an alert is thrown.\n",
    "\n",
    "Imutils and dlib can track the face and cv2 can access the webcam, so that's where I'll begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how does the imutils track eyes? it uses 6 points across each eye that looks like this:\n",
    "\n",
    "        1*   2*\n",
    "    0*  (    )  3*\n",
    "        5*   4*\n",
    "\n",
    "in order to determine when an eye is closed, we can use the eye aspect ratio (EAR), which can be computed using the Euclidean distance between points across from one another. the formula for EAR can be determined with the following formula: EAR = (‖p1 - p5‖ + ‖p2 - p4‖) / 2 * ‖p0 - p3‖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "\tA = distance.euclidean(eye[1], eye[5])\n",
    "\tB = distance.euclidean(eye[2], eye[4])\n",
    "\tC = distance.euclidean(eye[0], eye[3])\n",
    "\tEAR = (A + B) / (2.0 * C)\n",
    "\treturn EAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some other solutions to this problem also incorporate mouth aspect ratio (MAR) in order to track yawns. However, as a simple solution, I think that eye aspect ratio is a good enough predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.20 # this threshold will be used later to compare EAR to - if EAR is below the threshold, the frame will be flagged.\n",
    "frame_check = 20 # number of consecutive frames to be checked for EAR < thresh\n",
    "detect = dlib.get_frontal_face_detector()\n",
    "predict = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, I'll use imutils to get the indeces of the left and right eye, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have all of the required components for this problem, we can set up a while loop for the webcam and continous detection/computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "flag=0\n",
    "\n",
    "while True:\n",
    "\tret,frame = cap.read()\n",
    "\tframe = imutils.resize(frame, width=450)\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\tsubjects = detect(gray, 0)\n",
    "\tcv2.putText(frame, \"PRESS 'q' TO EXIT\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 3) \n",
    "\tfor subject in subjects:\n",
    "\t\tshape = predict(gray, subject)\n",
    "\t\tshape = face_utils.shape_to_np(shape) #converting to NumPy Array\n",
    "\t\tleftEye = shape[lStart:lEnd]\n",
    "\t\trightEye = shape[rStart:rEnd]\n",
    "\t\tleftEAR = eye_aspect_ratio(leftEye)\n",
    "\t\trightEAR = eye_aspect_ratio(rightEye)\n",
    "\t\tEAR = (leftEAR + rightEAR) / 2.0\n",
    "\t\tleftEyeHull = cv2.convexHull(leftEye)\n",
    "\t\trightEyeHull = cv2.convexHull(rightEye)\n",
    "\t\tcv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\tcv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\tif EAR < thresh:\n",
    "\t\t\tflag += 1 \n",
    "\t\t\tprint (flag)\n",
    "\t\t\tif flag >= frame_check:\n",
    "\t\t\t\tcv2.putText(frame, \"****************ALERT!****************\", (10, 215),\n",
    "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\t\t\t\t# cv2.putText(frame, \"****************ALERT!****************\", (10,325),\n",
    "\t\t\t\t# \tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\t\t\t\t# cv2.putText(frame, \"DROWSINESS ALERT!\", (270, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\t\t\t\t#print (\"Drowsy\")\n",
    "\t\telse:\n",
    "\t\t\tflag = 0\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](example.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One factor in the nuance of this problem is, drowsiness isn't always represented physically. Drowsiness doesn't only make people fall asleep, but also leads to longer reaction times, shorter focus times, and overall lower spatial awareness. While the answer that I have here scratches the surface of detecting drowsiness, the addition of a heart rate monitor might increase efficacy of this solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
